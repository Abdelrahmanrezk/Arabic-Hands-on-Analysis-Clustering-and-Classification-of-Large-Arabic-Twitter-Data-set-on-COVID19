{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg_model = pickle.load(open(\"ml_models/models_weights/logistic_regression/logistic_regression_93tr_91ts.sav\", \"rb\"))\n",
    "tf_idf_model = pickle.load(open(\"ml_models/models_weights/tf_idf/tf_idf_vectorizer_50000_fetures.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_read = 'csv_down_files/preprocessed/COVID19-tweetID-2020-01/' # January Month of 2020 \n",
    "path_to_save = 'csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/' # Classify each file tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small files functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-14.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-01.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-02.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-03.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-04.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-05.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-06.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-07.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-08.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-09.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-10.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-11.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-12.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-13.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-15.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-16.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-17.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-18.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-19.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-20.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-21.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-22.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-23.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-24.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-25.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-26.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-27.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-28.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-29.csv\n",
      "csv_down_files/classifed_tweeets_of_each_file/COVID19-tweetID-2020-01/csv/COVID19-tweetID-2020-01-30.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "304418"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify_tweets_of_file(file_path, df_file):  \n",
    "    tweets = list(df_file['full_text'])\n",
    "    tweets_features = tf_idf_model.transform(tweets)\n",
    "    tweets_features_array = tweets_features.toarray()\n",
    "    predict = log_reg_model.predict(tweets_features_array)\n",
    "    classifed_dict    = {'tweet_text': tweets, 'class': predict}\n",
    "    classifed_data    = pd.DataFrame(classifed_dict)\n",
    "    return classifed_data\n",
    "\n",
    "\n",
    "\n",
    "def read_direction_to_classify(direction_path, path_to_save):\n",
    "    try:\n",
    "        all_path_files                          = os.listdir(direction_path)\n",
    "        cls = []\n",
    "        for file_path in all_path_files:\n",
    "            file_path                           = direction_path + file_path\n",
    "            print((file_path))\n",
    "            df_file   = pd.read_csv(file_path, error_bad_lines=False, encoding='utf-8', engine='python')\n",
    "            cls.extend(df_file['class'])\n",
    "#             classify_tweets                     = classify_tweets_of_file(file_path, df_file)\n",
    "#             file_path                           = file_path.split('/')\n",
    "#             file_name                           = file_path[3]\n",
    "#             path_to_save_excel = path_to_save\n",
    "#             path_to_save_csv = path_to_save + \"csv/\" + file_name\n",
    "            \n",
    "#             path_to_save_excel = path_to_save_excel + \"excel/\" + file_name\n",
    "#             path_to_save_excel = path_to_save_excel.replace(\".csv\", '.xlsx')\n",
    "#             classify_tweets.to_csv(path_to_save_csv, index=False)\n",
    "#             classify_tweets.to_excel(path_to_save_excel, index=False)\n",
    "    except Exception as e:\n",
    "        file = open(\"logs/direction_and_file_handleing_file.log\",\"+a\")\n",
    "        file.write(\"This error related to function read_direction_preprocess of direction_and_file_handleing file in direction config_files n\"\n",
    "                   + str(e) + \"\\n\" + \"#\" *99 + \"\\n\") # \"#\" *99 as separated lines\n",
    "    return cls\n",
    "\n",
    "cls = read_direction_to_classify(path_to_save, path_to_save)\n",
    "len(cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big files functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def big_file_classify(df_file, file_name, path_to_save):\n",
    "#     tweets = list(df_file['full_text'])\n",
    "#     df_len = len(df_file)\n",
    "#     to_index = 0\n",
    "#     prdict_all = []\n",
    "#     while to_index <= df_len:\n",
    "#         part_of_tweets = tweets[to_index: to_index + 10000]\n",
    "#         to_index = to_index + 10000\n",
    "#         tweets_features = tf_idf_model.transform(part_of_tweets)\n",
    "#         tweets_features_array = tweets_features.toarray()\n",
    "#         predict = log_reg_model.predict(tweets_features_array)\n",
    "#         prdict_all.extend(predict)\n",
    "#     classifed_dict    = {'tweet_text': tweets, 'class': prdict_all}\n",
    "#     classifed_data    = pd.DataFrame(classifed_dict)\n",
    "#     path_to_save_csv = path_to_save + \"csv/\" + file_name\n",
    "#     file_name = file_name.replace('.csv', '.xlsx')\n",
    "#     path_to_save_excel = path_to_save + \"excel/\" + file_name\n",
    "#     classifed_data.to_csv(path_to_save_csv, index=False)\n",
    "#     classifed_data.to_excel(path_to_save_excel, index=False)\n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def read_direction_big_files(direction_path, path_to_save):\n",
    "#     try:\n",
    "#         all_path_files                          = os.listdir(direction_path)\n",
    "#         for file_path in all_path_files:\n",
    "#             file_path                           = direction_path + file_path\n",
    "#             print((file_path))\n",
    "#             df_file   = pd.read_csv(file_path, error_bad_lines=False, encoding='utf-8', engine='python')\n",
    "            \n",
    "#             file_path                           = file_path.split('/')\n",
    "#             file_name                           = file_path[3]\n",
    "#             big_file_classify(df_file, file_name, path_to_save)\n",
    "#     except Exception as e:\n",
    "#         file = open(\"logs/direction_and_file_handleing_file.log\",\"+a\")\n",
    "#         file.write(\"This error related to function read_direction_preprocess of direction_and_file_handleing file in direction config_files n\"\n",
    "#                    + str(e) + \"\\n\" + \"#\" *99 + \"\\n\") # \"#\" *99 as separated lines\n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read_direction_big_files(path_to_read, path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_tweets(direction_read):\n",
    "    try:\n",
    "        all_path_files              = os.listdir(direction_read)\n",
    "        all_files_tweets            = []\n",
    "        all_files_tweets_predicited = []\n",
    "        \n",
    "        for file_path in all_path_files:\n",
    "            file_path           = direction_read + file_path\n",
    "            df_file             = pd.read_csv(file_path, lineterminator='\\n', error_bad_lines=False, encoding='utf-8')\n",
    "            all_files_tweets    += list(df_file['tweet_text'])\n",
    "            all_files_tweets_predicited    += list(df_file['class'])\n",
    "        dicts_df                = {\"tweet_text\": all_files_tweets, 'class':all_files_tweets_predicited}\n",
    "        df_file                 = pd.DataFrame(dicts_df)\n",
    "              \n",
    "    except Exception as e:\n",
    "        file = open(\"logs/get_tweet_to_classify.log\",\"+a\")\n",
    "        file.write(\"This error related to function get_random_number_of_tweets of get_tweet_to_classify file in direction config_files n\"\n",
    "                   + str(e) + \"\\n\" + \"#\" *99 + \"\\n\") # \"#\" *99 as separated lines\n",
    "    return df_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = path_to_save + 'csv/'\n",
    "df_file = get_all_tweets(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>منظمه الصحه العالميه تدعو  الصين التحقيق في مص...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>الصحه العالميه   ظهور  كورونا  جديد في الصين</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الصحه العالميه   ظهور  كورونا  جديد في الصين\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>وضع الاطفال الشاشات صباحا يزيد خطر تعرضهم لمشك...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الصحه العالميه تحذر ظهور  كورونا  جديد في الص...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  class\n",
       "0  منظمه الصحه العالميه تدعو  الصين التحقيق في مص...      1\n",
       "1     الصحه العالميه   ظهور  كورونا  جديد في الصين        1\n",
       "2   الصحه العالميه   ظهور  كورونا  جديد في الصين\\...      1\n",
       "3  وضع الاطفال الشاشات صباحا يزيد خطر تعرضهم لمشك...      0\n",
       "4   الصحه العالميه تحذر ظهور  كورونا  جديد في الص...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "cls = np.array(cls)\n",
    "cls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128971"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_1 = np.sum(cls == 1)\n",
    "class_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175447"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_0 = np.sum(cls == 0)\n",
    "class_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of coronavirus Tweets:  128971\n",
      "Number of other Tweets:  175447\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of coronavirus Tweets: \", class_1)\n",
    "print(\"Number of other Tweets: \", class_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram = [0, 1]\n",
    "\n",
    "labels= 'Class 1 Coronavirus', 'Class 0 General'\n",
    "sizes = [class_1, class_0]\n",
    "explode= (0, 0.1)  \n",
    "fig1, ax1= plt.subplots()\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.title(\"Coronavirus Vs General Tweets\")\n",
    "plt.savefig('test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

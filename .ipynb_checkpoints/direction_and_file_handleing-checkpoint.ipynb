{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import nltk\n",
    "import string\n",
    "import chardet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autocorrect import spell\n",
    "from nlppreprocess import NLP\n",
    "from textblob import Word, TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords, webtext\n",
    "from tashaphyne.stemming import ArabicLightStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_file_preprocess(Inputfile):\n",
    "    '''\n",
    "    The function used to read CSV file and convert the read tweets from this file to list.\n",
    "    After that the function path the list of tweets to the Cleaning Arabic pipeline.\n",
    "    Argument:\n",
    "        Inputfile: The file path that you need to read.\n",
    "    Return:\n",
    "        The tweets contained in this file as one list that contain all tweets.\n",
    "    \n",
    "    '''\n",
    "    df_file = pd.read_csv(Inputfile,lineterminator='\\n', error_bad_lines=False, encoding='utf-8')\n",
    "    print(len(df_file))\n",
    "    full_text_list = list(df_file['full_text'])\n",
    "    full_text_list_preprocessed = arabic_pip_line(full_text_list)\n",
    "    return full_text_list_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_direction_preprocess(direction_path):\n",
    "    '''\n",
    "    Loop over each direction and path each file in this direction to get all tweets,\n",
    "    then to save the new preprocessed data which cleaned tweets we have make another dierction,\n",
    "    to save the data in for each file create the same file name and save in another dierction.\n",
    "    Argument:\n",
    "        direction_path: The dierction you aimed to read files from.\n",
    "    '''\n",
    "    try:\n",
    "        all_path_files = os.listdir(direction_path)\n",
    "        for file_path in all_path_files:\n",
    "            file_path = direction_path + file_path\n",
    "            full_text_list_preprocessed = one_file_preprocess(file_path)\n",
    "            file_path = file_path.split('/')\n",
    "            file_path[1] = 'preprocessed'\n",
    "            file_path = '/'.join(file_path)\n",
    "            dict_full_text_list_preprocessed = {'full_text': full_text_list_preprocessed}\n",
    "            dict_full_text_list_preprocessed = pd.DataFrame(dict_full_text_list_preprocessed)\n",
    "            file_path = file_path[:-7]\n",
    "            file_path = file_path + 'csv'\n",
    "            dict_full_text_list_preprocessed.to_csv(file_path, index=False)\n",
    "    except Exception as e:\n",
    "        file = open(\"logs/direction_and_file_handleing_file.log\",\"+a\")\n",
    "        file.write(\"This error related to function read_direction_preprocess of direction_and_file_handleing file n\"\n",
    "                   + str(e) + \"\\n\" + \"#\" *99 + \"\\n\") # \"#\" *99 as separated lines\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_direction_analysis(direction, replace_with):\n",
    "    '''\n",
    "    Use one default direction and replace this with others like below:\n",
    "    \"COVID-19-Arabic-Tweets-Dataset/COVID19-tweetID-2020-01/\" - to\n",
    "    \"COVID-19-Arabic-Tweets-Dataset/COVID19-tweetID-2020-02/\"\n",
    "    Argument:\n",
    "        direction: the folder path you need to change.\n",
    "        replace_with: replace the folder name.\n",
    "    reutrn:\n",
    "        The new directions we need to work with\n",
    "    '''\n",
    "    direction = re.sub('([1-9]/)', replace_with , direction)\n",
    "    return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_file_analysis(Inputfile):\n",
    "    '''\n",
    "    The function to get all rows in file in one list\n",
    "    Argument:\n",
    "        Inputfile: The file path we aimed to get all of its strings.\n",
    "    Return:\n",
    "        full_text_list: retrun these strings as a list each index represent one string.\n",
    "    '''\n",
    "    df_file = pd.read_csv(Inputfile,  lineterminator='\\n',  error_bad_lines=False, encoding='utf-8')\n",
    "    full_text_list = list(df_file['full_text'])\n",
    "    return full_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_direction_analysis(direction_path, convert_list = \"to_list_of_words\"):\n",
    "    '''\n",
    "    Loop over each direction and path each file in this direction, then get the tweets of one path as a list,\n",
    "    then get all of the words of this list, and extend this list to contain all words in all files of one direction.\n",
    "    Argument:\n",
    "        direction_path: The dierction you aimed to read files from.\n",
    "        convert_list: default argument used when it's required.\n",
    "    Return:\n",
    "        All the words in all files of one direction\n",
    "    '''\n",
    "    try:\n",
    "        all_path_files = os.listdir(direction_path)\n",
    "        all_words_of_direction = []\n",
    "        word_list = []\n",
    "        for file_path in all_path_files:\n",
    "            file_path = direction_path + file_path\n",
    "            full_text_list = one_file_analysis(file_path)\n",
    "            \n",
    "            if convert_list == \"to_list_of_words\":\n",
    "                full_text_list = convert_list_of_strings_to_list_of_words(full_text_list)\n",
    "            \n",
    "            # extend the list of words of previous files to new words of another file\n",
    "            all_words_of_direction.extend(full_text_list) \n",
    "            \n",
    "#             print(\"The total words now are: \", len(all_words_of_direction))\n",
    "#     pull the error to logs direction\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"=\"*50)\n",
    "        file = open(\"logs/direction_and_file_handleing.log\",\"+a\")\n",
    "        file.write(\"This error related to function read_direction_analysis of direction_and_file_handleing file n\" \n",
    "                   + str(e) + \"\\n\" + \"#\" *99 + \"\\n\") # \"#\" *99 as separated lines\n",
    "    return all_words_of_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_dropna_separate_split(df_file, tweet_text, tweet_class):\n",
    "    df_file.dropna(inplace=True)\n",
    "    df_file = df_file.sample(frac=1).reset_index(drop=True) ## Shuffle tweets\n",
    "    print(\"The number of rows in this file are: \", len(df_file))\n",
    "    print(\"The columns are: \", df_file.columns)\n",
    "    print(\"The number of class 1 which represent this tweet talks about cron: \", len(df_file[df_file['class'] == 1]))\n",
    "    print(\"The number of class 0 which represent this tweet not talks about cron: \", len(df_file[df_file['class'] == 0]))\n",
    "    \n",
    "      # Separate tweets and target classification \n",
    "    tweets_text = df_file[tweet_text] \n",
    "    tweets_class = df_file[tweet_class]\n",
    "\n",
    "    ## Split data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(tweets_text, tweets_class, test_size=.2)\n",
    "    X_train[:5]\n",
    "\n",
    "    # Display \n",
    "    print(\"Our training data now are: \" + str(len(X_train))  + \" Tweets\")\n",
    "    print(\"Our testing data now are: \" + str(len(X_test))  + \" Tweets\")\n",
    "    print(\"Our training data now are: \" + str(len(y_train))  + \" labels\")\n",
    "    print(\"Our testing data now are: \" + str(len(y_test))  + \" labels\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import nltk\n",
    "import string\n",
    "import chardet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autocorrect import spell\n",
    "from nlppreprocess import NLP\n",
    "from textblob import Word, TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords, webtext\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "from configs import *\n",
    "%run direction_and_file_handleing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_list_of_strings_to_list_of_words(text):\n",
    "    '''\n",
    "    A function used to convert list of strings to be list of all words of these strings like:\n",
    "    ['تليفون جيد',\n",
    " 'ايباد ميني فور من افضل الايبادات في السوق وسعره كان مغري',]\n",
    "    return:\n",
    "        list of all words in all string like:\n",
    "        ['تليفون', 'جيد'] and so on\n",
    "    '''\n",
    "    word_list = []\n",
    "    for i in text:\n",
    "        i = i.split()\n",
    "        word_list.extend(i)\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_list_all_words(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of lists each of them are words\n",
    "    return:\n",
    "        one list contain all words\n",
    "    '''\n",
    "    updated_list = []\n",
    "    [updated_list.extend(li) for li in text_list]\n",
    "    return updated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def one_string_lower_conversation(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        text as string of words\n",
    "    return:\n",
    "        lower of this string\n",
    "    '''\n",
    "    return sentence.lower()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def all_string_lower_conversation(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings and each of these strings does contain some of words\n",
    "    return:\n",
    "        lower each string in this list\n",
    "    '''\n",
    "    text_list = [one_string_lower_conversation(sentence) for sentence in text_list]\n",
    "    return text_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def one_string_remove_diacritics(sentence):\n",
    "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    sentence = re.sub(noise, '', sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def all_string_remove_diacritics(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings\n",
    "    return:\n",
    "        list of string without special chars from Arabic language\n",
    "    '''\n",
    "    text_list = [one_string_remove_diacritics(sentence) for sentence in text_list]\n",
    "    return text_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_string_remove_punctuation(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        string of words\n",
    "    reutrn:\n",
    "        string without punctuation like [.!?] and others\n",
    "    '''\n",
    "    sentence = re.sub('[@]\\w+', '', sentence)\n",
    "    sentence = re.sub(r'((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', '', sentence)\n",
    "    sentence = sentence.split(' ')\n",
    "    strs = ''\n",
    "    punctuations = string.punctuation\n",
    "    for word in sentence:\n",
    "#         word = re.sub(r'(.)\\1+', r'\\1', word) # remove repated chars\n",
    "        word = re.sub('[^\\w\\s+]',' ',word)\n",
    "        if len(word) > 1 and not (word[0] >= 'a' and word[0] < 'z' or word[0] >= 'A' and word[0] < 'Z'):\n",
    "            strs += word + ' '\n",
    "    translator = str.maketrans('', '', punctuations)\n",
    "    strs.translate(translator)\n",
    "    return strs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def all_strings_remove_punctuation(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings \n",
    "    reutrn:\n",
    "        list of strings without punctuation like [.!?] and others\n",
    "    '''\n",
    "    text_list = [one_string_remove_punctuation(sentence) for sentence in text_list]\n",
    "    return text_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def one_string_normalize_arabic(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        string of words\n",
    "    return:\n",
    "        string of words but standardize the words\n",
    "    '''\n",
    "    sentence = re.sub(\"[إأآا]\", \"ا\", sentence)\n",
    "    sentence = re.sub(\"ى\", \"ي\", sentence)\n",
    "    sentence = re.sub(\"ؤ\", \"ء\", sentence)\n",
    "    sentence = re.sub(\"ئ\", \"ء\", sentence)\n",
    "    sentence = re.sub(\"ة\", \"ه\", sentence)\n",
    "    sentence = re.sub(\"گ\", \"ك\", sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def all_string_normalize_arabic(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings\n",
    "    return:\n",
    "        list of strings but replace some of chars  like ة to ه Arabic words\n",
    "    '''\n",
    "    text_list = [one_string_normalize_arabic(sentence) for sentence in text_list]\n",
    "    return text_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def one_string_tokenization(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        String of words\n",
    "    return:\n",
    "        list of words\n",
    "    '''\n",
    "    sentence = word_tokenize(sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def all_string_tokenization(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of Strings\n",
    "    return:\n",
    "        list of strings and every string is list of words\n",
    "    '''\n",
    "    text_list = [one_string_tokenization(sentence) for sentence in text_list]\n",
    "    return text_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def one_string_un_tokenization(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of words\n",
    "    return:\n",
    "        string of words\n",
    "    '''\n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def all_string_un_tokenization(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of words\n",
    "    return:\n",
    "        string of words\n",
    "    '''\n",
    "    text_list = [one_string_un_tokenization(sentence) for sentence in text_list]\n",
    "    return text_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def one_string_spelling_correction(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        string of words\n",
    "    return:\n",
    "        string of correct words\n",
    "    '''\n",
    "    \n",
    "    sentence = str(TextBlob(sentence).correct())\n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def all_string_spelling_correction(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings each of them are some of words\n",
    "    return:\n",
    "        list of correct strings\n",
    "    '''\n",
    "    text_list = [one_string_spelling_correction(sentence) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def one_string_steming(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        String of words\n",
    "    return:\n",
    "        list of words with steming which the root of the word\n",
    "    '''\n",
    "    sentence = one_string_tokenization(sentence)\n",
    "    stemmer = ISRIStemmer()\n",
    "    sentence = [stemmer.stem(word) for word in sentence]\n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def all_string_steming(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings\n",
    "    return:\n",
    "        list of strings with steming which the root of the word in each string\n",
    "    '''\n",
    "    text_list = [one_string_steming(sentence) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def one_string_Lemmatizing(sentence, language):\n",
    "    '''\n",
    "    Argument:\n",
    "        String of words\n",
    "    return:\n",
    "        list of words with Lemmatizing\n",
    "    '''\n",
    "    sentence = one_string_tokenization(sentence)\n",
    "    stemmer = ArabicLightStemmer()\n",
    "    sentence = [stemmer.light_stem(word) for word in sentence]\n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def all_string_Lemmatizing(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings\n",
    "    return:\n",
    "        list of strings with steming which the root of the word in each string\n",
    "    '''\n",
    "    text_list = [one_string_Lemmatizing(sentence) for sentence in text_list]\n",
    "    return text_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def convert_file_of_stop_words_to_list(file_dir):\n",
    "    '''\n",
    "    Argument:\n",
    "        file with stop words\n",
    "    return:\n",
    "        list of these stop words\n",
    "    '''\n",
    "    \n",
    "    stop_words_designed = []\n",
    "    with open(file_dir, 'r') as file:\n",
    "        file = file.readlines()\n",
    "        file = \"\".join(file)\n",
    "        file = re.sub('[\\[\\]\\'\\\",]', '', file)\n",
    "        stop_words_designed = file.split()\n",
    "    return stop_words_designed\n",
    "    \n",
    "\n",
    "def one_string_stop_words(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        string of words\n",
    "    return:\n",
    "        remove stop words from this string like this, did\n",
    "        but other words like not, no dont remove\n",
    "    '''\n",
    "\n",
    "    file_dir1 =  'stop_words/nltk_stop_words_handle.txt'\n",
    "    file_di2 = 'stop_words/stop_list1.txt'\n",
    "    file_di3 ='stop_words/updated_stop_words.txt'\n",
    "\n",
    "    stop_words_designed = []\n",
    "    stop_words_designed.extend(convert_file_of_stop_words_to_list(file_di2))\n",
    "\n",
    "    stop_words_designed = set(stop_words_designed)\n",
    "    stop_words_designed = list(stop_words_designed)\n",
    "    arabic_stop_words_designed = convert_file_of_stop_words_to_list(file_di3)\n",
    "\n",
    "    stop_words = arabic_stop_words_designed \n",
    "    sentence = sentence.split(' ')\n",
    "    updated_sentence = ''\n",
    "    for word in sentence:\n",
    "        if word not in stop_words:\n",
    "            updated_sentence += word + ' '\n",
    "    return updated_sentence\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def all_string_stop_words(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of string\n",
    "    return:\n",
    "        list of string without stop words\n",
    "    '''        \n",
    "    text_list = [one_string_stop_words(sentence) for sentence in text_list]\n",
    "    return text_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arabic_pip_line(text_list):\n",
    "    text_list = all_string_remove_diacritics(text_list)\n",
    "    text_list = all_strings_remove_punctuation(text_list)\n",
    "    text_list = all_string_normalize_arabic(text_list)\n",
    "    text_list = all_string_stop_words(text_list)\n",
    "\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# direction_path1 = \"csv_down_files/COVID-19-Arabic-Tweets-Dataset/COVID19-tweetID-2020-01/\"\n",
    "# _ = read_direction(direction_path1)\n",
    "# direction_path2 = \"csv_down_files/COVID-19-Arabic-Tweets-Dataset/COVID19-tweetID-2020-02/\"\n",
    "# _ = read_direction(direction_path2)\n",
    "# direction_path3 = \"csv_down_files/COVID-19-Arabic-Tweets-Dataset/COVID19-tweetID-2020-03/\"\n",
    "# _ = read_direction(direction_path3)\n",
    "# direction_path4 = \"csv_down_files/COVID-19-Arabic-Tweets-Dataset/COVID19-tweetID-2020-04/\"\n",
    "# _ = read_direction(direction_path4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

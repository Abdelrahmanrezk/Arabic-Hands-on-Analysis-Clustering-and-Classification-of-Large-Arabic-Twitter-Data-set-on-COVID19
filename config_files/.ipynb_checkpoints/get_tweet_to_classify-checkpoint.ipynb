{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import chardet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from configs import *\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate_number_of_tweet_each_file(direction_read, direction_save, number_of_tweets):\n",
    "    '''\n",
    "    Loop over each direction and path each file in this direction to get 100 tweets.\n",
    "    Argument:\n",
    "        direction_read: Which direction we will read files from.\n",
    "        direction_save: After shuffle process and get first number_of_tweets \n",
    "        from each file save all these tweets, in one file which will be classifed later for our model.\n",
    "    Return:\n",
    "        True and before of that save the total tweets we get from each file as sample in one file.\n",
    "    '''\n",
    "    try:\n",
    "        all_path_files = os.listdir(direction_read)\n",
    "        list_3000_tweet = []\n",
    "        for file_path in all_path_files:\n",
    "            file_path = direction_read + file_path\n",
    "            df_file = pd.read_csv(file_path, error_bad_lines=False, encoding='utf-8')\n",
    "            df_file = df_file.sample(frac=1).reset_index(drop=True)\n",
    "            list_3000_tweet +=(list(df_file['full_text'][:number_of_tweets]))\n",
    "            dict_300_tweets = {'sample_3000_tweets': list_3000_tweet}\n",
    "            data_frame = pd.DataFrame(dict_300_tweets)\n",
    "            data_frame.to_csv(direction_save + '3000_tweet_not_classified.csv', index=False)\n",
    "            data_frame.to_excel(direction_save + '3000_tweet_not_classified.xlsx', index=False)\n",
    "    except Exception as e:\n",
    "        file = open(\"../logs/3000_tweet_file.log\",\"+a\")\n",
    "        file.write(\"This error related to function separate_100_tweet_each_file of 3000_tweet file n\"\n",
    "                   + str(e) + \"\\n\" + \"#\" *99 + \"\\n\") # \"#\" *99 as separated lines\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First Direction - 100 because we have 30 file and we need sample of 3000\n",
    "# separate_number_of_tweet_each_file(preprocessed_direction_path_tweets, analysis_direction_path_tweets, 3000)\n",
    "\n",
    "# Second Direction - 200 because we have 15 fileand we need sample of 3000\n",
    "# preprocessed_direction_path_tweets = handle_direction_analysis(preprocessed_direction_path_tweets, '2/')\n",
    "# analysis_direction_path_tweets = handle_direction_analysis(analysis_direction_path_tweets, '2/')\n",
    "\n",
    "# separate_number_of_tweet_each_file(preprocessed_direction_path_tweets, analysis_direction_path_tweets, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_number_of_tweets(direction_read, direction_save, number_of_tweets):\n",
    "    \"\"\"\n",
    "    Function to read all tweets of all files in one direction into one list, \n",
    "    then take random tweets from this list, also for each tweet make some regular expression,\n",
    "    like replace \\n with just space to make the whole tweets in one line,\n",
    "    then save the result in CSV and  XLS file.\n",
    "    Arguemtn:\n",
    "        direction_read: Which direction you will read files from.\n",
    "        direction_save: After you take the random tweets and make a data frame of it which direction you need to save.\n",
    "        number_of_tweets: which number of tweets at all you need.\n",
    "    Return:\n",
    "        True if the process success other send the error to log file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_path_files = os.listdir(direction_read)\n",
    "        all_files_tweets = []\n",
    "        for file_path in all_path_files:\n",
    "            file_path = direction_read + file_path\n",
    "            df_file = pd.read_csv(file_path, lineterminator='\\n', error_bad_lines=False, encoding='utf-8')\n",
    "            df_file.drop_duplicates(inplace = True)\n",
    "            df_file.reset_index(inplace=True, drop=True)\n",
    "            all_files_tweets += list(df_file['full_text'])\n",
    "\n",
    "        dicts_df = {\"tweet_text\": all_files_tweets}\n",
    "        df_file = pd.DataFrame(dicts_df)\n",
    "        df_file.drop_duplicates(inplace = True)\n",
    "        df_file.reset_index(drop=True, inplace=True)\n",
    "        all_files_tweets = list(df_file['tweet_text'])\n",
    "\n",
    "        random_indexes = random.sample(range(len(all_files_tweets)), number_of_tweets)\n",
    "        all_files_tweets = [all_files_tweets[tweet_index] for tweet_index in random_indexes]\n",
    "        all_files_tweets_last = []\n",
    "\n",
    "        for tweet in all_files_tweets:\n",
    "            tweet = re.sub(\"(\\\\n)+\", \" \", tweet)\n",
    "#             tweet = tweet.replace(\"  \", \"\")\n",
    "            tweet = tweet.strip()\n",
    "            all_files_tweets_last.append(tweet)\n",
    "\n",
    "        tweet_text = {'class': 0, 'tweet_text': all_files_tweets_last}\n",
    "        data_frame = pd.DataFrame(tweet_text)\n",
    "#         data_frame.to_csv(direction_save + 'tweet_not_classified_2000_26_12.csv', index=False)\n",
    "#         data_frame.to_excel(direction_save + 'tweet_not_classified_2000_26_12.xlsx', index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        file = open(\"../logs/get_random_number_of_tweets.log\",\"+a\")\n",
    "        file.write(\"This error related to function get_random_number_of_tweets of get_tweet_to_classify file n\"\n",
    "                   + str(e) + \"\\n\" + \"#\" *99 + \"\\n\") # \"#\" *99 as separated lines\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First Direction - 100 because we have 30 file and we need sample of 3000\n",
    "# df1 = get_random_number_of_tweets(preprocessed_direction_path_tweets, analysis_direction_path_tweets, 2000)\n",
    "\n",
    "# # # Second Direction - 200 because we have 15 fileand we need sample of 3000\n",
    "# preprocessed_direction_path_tweets = handle_direction_analysis(preprocessed_direction_path_tweets, '2/')\n",
    "# analysis_direction_path_tweets = handle_direction_analysis(analysis_direction_path_tweets, '2/')\n",
    "\n",
    "# df2 = get_random_number_of_tweets(preprocessed_direction_path_tweets, analysis_direction_path_tweets, 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arabic plot words handle\n",
    "# !pip install arabic_reshaper\n",
    "# !pip install python-bidi\n",
    "from __future__ import unicode_literals\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "import os\n",
    "import collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from configs import *\n",
    "# Features Extraction word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Dimension reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from direction_and_file_handleing import *\n",
    "from cleaning import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dir_max_all_tweets(preprocessed_direction_path_tweets):\n",
    "    all_tweets_of_direction = read_direction_analysis(preprocessed_direction_path_tweets, \"none\")\n",
    "    print(\"The number of tweets in our direction is: \", len(all_tweets_of_direction))\n",
    "    max_len_str = max([len(i.split()) for i in all_tweets_of_direction])\n",
    "    print(\"The greatest string of our reviews: \", max_len_str)\n",
    "    all_tweets_of_direction = [i.split() for i in all_tweets_of_direction] # split each review to list of words\n",
    "    print(\"The firt 5 tweets are: \", all_tweets_of_direction[:5])\n",
    "    return all_tweets_of_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets in our direction is:  80999\n",
      "The greatest string of our reviews:  63\n",
      "The firt 5 tweets are:  [['منظمه', 'الصحه', 'العالميه', 'تدعو', 'الصين', 'التحقيق', 'في', 'مصدر', 'فيروس', 'كورونا', 'الجديد', 'مع', 'ظهور', 'حاله', 'اصابه', 'في', 'تايلند', 'كيف', 'اكتشاف', 'الحاله', 'الجديده', 'كيف', 'تستعد', 'لمعالجه', 'الوضع', 'وما', 'فيروس', 'كورونا', 'التفاصيل', 'في'], ['الصحه', 'العالميه', 'ظهور', 'كورونا', 'جديد', 'في', 'الصين'], ['الصحه', 'العالميه', 'ظهور', 'كورونا', 'جديد', 'في', 'الصين', 'تبوك', 'تبوك_الان', 'صدي_تبوك'], ['وضع', 'الاطفال', 'الشاشات', 'صباحا', 'يزيد', 'خطر', 'تعرضهم', 'لمشكلات', 'في', 'النطق', 'العربيه'], ['الصحه', 'العالميه', 'تحذر', 'ظهور', 'كورونا', 'جديد', 'في', 'الصين', 'والحصيله', 'الاوليه', '59', 'اصابه', 'حتي', 'اللحظه', 'رفحاء', 'عرعر', 'طريف', 'العويقيله']]\n"
     ]
    }
   ],
   "source": [
    "all_tweets_of_direction1 = dir_max_all_tweets(preprocessed_direction_path_tweets)\n",
    "# new_direction = handle_direction_analysis(preprocessed_direction_path_tweets, '2/')\n",
    "# all_tweets_of_direction2 = dir_max_all_tweets(new_direction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_features = 300\n",
    "window_size = 5\n",
    "min_words_count = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_vec(text_list, number_of_features, window_size, min_words_count, model_bin_saved):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings and each string is list of words\n",
    "        size = extract 50 features for each word\n",
    "        window = 7 take the context of neighbour words\n",
    "        min_count = 1 consider each word that even repeated 1 time\n",
    "    return:\n",
    "        the word2vec model\n",
    "    '''\n",
    "    word_to_vec_model = Word2Vec(text_list, size =number_of_features, window = window_size, min_count=min_words_count, sg = 1) \n",
    "    print(\"Our word2vec model: \", word_to_vec_model)\n",
    "    print(\"The number of frequent words of our data: \", len(word_to_vec_model.wv.vocab)) # the frequent words\n",
    "    # save model\n",
    "    word_to_vec_model.save('../ml_models/models_weights/word2vec/' + model_bin_saved)\n",
    "\n",
    "    # load model\n",
    "    word_to_vec_model = Word2Vec.load('../ml_models/models_weights/word2vec/'+ model_bin_saved)\n",
    "    return word_to_vec_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_words(url, word_to_vec_model):\n",
    "\n",
    "    w2c = dict()\n",
    "    for item in word_to_vec_model.wv.vocab:\n",
    "        if not item.isdigit() and len(item) > 2 and not \"_\" in item and not (item[0] >= 'a' and item[0] <= 'z'):\n",
    "            w2c[item]=word_to_vec_model.wv.vocab[item].count\n",
    "    dicts = {'word2vec_model_words': list(w2c.values()), 'word2vec_model_frequency': list(w2c.keys())}\n",
    "    df = pd.DataFrame(dicts)\n",
    "    url = url + 'csv/word2vec_model_words.csv'\n",
    "    df.to_csv(url, index=False)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # direction 1\n",
    "# word_to_vec_model = word_to_vec(all_tweets_of_direction1, number_of_features, \n",
    "#                                 window_size, min_words_count, \"word2vec_anaysis_file_for_tablue.bin\")\n",
    "# _ = save_words(analysis_direction_path_tweets, word_to_vec_model)\n",
    "word_to_vec_model = Word2Vec.load('../ml_models/models_weights/word2vec/word2vec_anaysis_file_for_tablue.bin' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word2vec_model_words</th>\n",
       "      <th>word2vec_model_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2687</td>\n",
       "      <td>منظمه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540</td>\n",
       "      <td>الصحه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6232</td>\n",
       "      <td>العالميه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2947</td>\n",
       "      <td>تدعو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>213289</td>\n",
       "      <td>الصين</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word2vec_model_words word2vec_model_frequency\n",
       "0                  2687                    منظمه\n",
       "1                 10540                    الصحه\n",
       "2                  6232                 العالميه\n",
       "3                  2947                     تدعو\n",
       "4                213289                    الصين"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(analysis_direction_path_tweets+ 'csv/word2vec_model_words.csv')\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('وزاره', 0.5062141418457031), ('الصحيه', 0.4807060956954956), ('حاله', 0.4573962688446045), ('منظمه', 0.4357641041278839), ('الاجراءات', 0.41532406210899353), ('صحه', 0.4090246558189392), ('العالميه', 0.4052422046661377), ('طوارء', 0.40076807141304016), ('مصابه', 0.39856642484664917), ('اصابه', 0.394173264503479)]\n",
      "[('بفيروس', 0.5231550931930542), ('فيروس_كورونا_الجديد', 0.41551750898361206), ('حاله', 0.39330053329467773), ('اصابه', 0.39247965812683105), ('الاصابه', 0.3906317353248596), ('تسجيل', 0.3867267370223999), ('الفيروس', 0.3818226456642151), ('فيروس_كورونا', 0.37590306997299194), ('المصابين', 0.36702844500541687), ('الغامض', 0.36474013328552246)]\n",
      "Feature of some words\n",
      " [ 7.60068536e-01 -1.16706304e-01 -3.50713521e-01 -5.94351292e-02\n",
      " -2.05351949e-01  6.97977021e-02 -3.75328422e-01 -2.43842110e-01\n",
      " -4.94518736e-03  5.45679852e-02  1.18231945e-01  2.37496406e-01\n",
      " -2.40674689e-01  3.87593418e-01 -7.45591298e-02 -2.71199569e-02\n",
      "  8.73693004e-02 -5.14979586e-02  4.71932028e-04 -2.54329771e-01\n",
      "  8.78553241e-02  2.25687772e-01  7.59825110e-02  2.18281969e-01\n",
      " -3.99358273e-01  7.90111646e-02  1.49991021e-01  6.69908598e-02\n",
      "  4.89513949e-02  2.36867204e-01  5.74147582e-01  2.78572202e-01\n",
      " -1.75068527e-01 -1.08381860e-01  1.75287113e-01  1.87063247e-01\n",
      " -3.52963656e-01  8.94976184e-02  3.47858369e-02  9.36991423e-02\n",
      " -1.80119015e-02  1.01154573e-01 -4.62261327e-02 -1.66735604e-01\n",
      "  7.69030005e-02 -2.85201464e-02  1.89537659e-01 -1.51500413e-02\n",
      " -4.06677052e-02  3.15488458e-01  3.92553627e-01  2.09154531e-01\n",
      "  4.19758797e-01 -1.58250600e-01  6.40598238e-02 -2.46890068e-01\n",
      " -7.17468113e-02  4.67245072e-01 -2.77611405e-01 -5.72200902e-02\n",
      " -2.45566159e-01 -2.87771374e-01  1.05865940e-01  9.64678451e-02\n",
      " -1.55100003e-01 -5.75460121e-02 -3.87863010e-01  2.71903247e-01\n",
      "  2.35146895e-01 -2.19324902e-01 -7.83582553e-02  6.79048821e-02\n",
      "  8.65277052e-02  8.93501937e-02  7.11034313e-02  9.19987932e-02\n",
      " -7.58815855e-02 -2.43711293e-01  7.22406954e-02 -8.06669891e-02\n",
      " -6.34745061e-02  2.98718661e-01  8.21403712e-02  4.07796532e-01\n",
      " -1.11648299e-01 -3.19597602e-01  3.80096547e-02 -3.96188557e-01\n",
      "  3.70154470e-01  2.40084790e-02 -1.51946947e-01 -2.07589075e-01\n",
      "  2.50622451e-01 -6.89102635e-02 -2.29451247e-02  9.01845992e-02\n",
      " -1.81231633e-01  3.56484279e-02 -3.04826736e-01  4.07106355e-02\n",
      "  2.88085938e-01  8.51858184e-02  1.73216745e-01  1.61353365e-01\n",
      "  1.02089919e-01 -1.02700397e-01 -3.79265308e-01 -1.38529718e-01\n",
      " -1.43897653e-01  9.66186821e-02  1.13873091e-02 -1.57865331e-01\n",
      " -2.88380962e-02  3.03141419e-02  1.96869537e-01  8.60569030e-02\n",
      " -2.57155269e-01 -1.78885087e-01  1.44512787e-01  1.25338053e-02\n",
      " -2.75673661e-02 -5.47975525e-02 -8.78549889e-02  1.91832632e-01\n",
      " -6.76735416e-02 -1.72178671e-01 -3.91535640e-01 -9.17289928e-02\n",
      "  5.09864949e-02  7.69431610e-03 -9.70477387e-02 -2.08720434e-02\n",
      " -4.82008964e-01 -1.01317903e-02 -2.03333497e-01  1.41164311e-03\n",
      " -1.71330318e-01  4.41309661e-02 -2.78124720e-01 -1.13384118e-02\n",
      "  1.91992503e-02 -1.36577860e-01  6.17416978e-01 -3.20243895e-01\n",
      " -7.13348463e-02  3.01102400e-01  9.01364197e-04 -1.49880936e-02\n",
      " -6.02254421e-02  2.16616288e-01  8.68588462e-02  1.57338101e-02\n",
      " -2.00204000e-01  9.92229134e-02  7.77637959e-02 -1.18294783e-01\n",
      " -3.25700372e-01 -3.15628022e-01 -5.53646870e-02 -4.97640550e-01\n",
      " -3.91035706e-01  2.14297920e-02 -1.97449148e-01 -1.94079742e-01\n",
      "  1.25654861e-01  2.61976480e-01  1.44269377e-01  1.71012804e-01\n",
      "  7.27079064e-02 -1.37053236e-01  3.55453253e-01  2.06277266e-01\n",
      " -1.52959302e-01 -1.55559227e-01 -1.84532285e-01  2.88683832e-01\n",
      "  2.18075365e-01 -5.91869524e-04 -1.00274846e-01 -1.88669339e-01\n",
      " -1.06645159e-01  3.59928794e-02  3.41142356e-01 -8.07908326e-02\n",
      " -4.47253257e-01  1.64401352e-01  1.98934555e-01  8.01360831e-02\n",
      " -2.49704290e-02 -2.25882083e-01 -2.46401191e-01  1.03996672e-01\n",
      " -1.16208658e-01 -1.09625980e-01  8.59897062e-02  9.82894376e-02\n",
      "  1.13158263e-01 -9.04934853e-03  3.55036199e-01 -7.19896555e-02\n",
      " -5.10646449e-03  1.61761921e-02 -1.27795011e-01 -2.46752471e-01\n",
      "  1.40926719e-01  1.90227434e-01 -1.68082789e-01 -7.31009319e-02\n",
      "  6.68656379e-02 -4.54344988e-01  1.87712103e-01  1.14163548e-01\n",
      " -2.45869234e-01 -2.69273985e-02  2.69602150e-01 -6.81449249e-02\n",
      " -2.13369668e-01 -1.61457911e-01 -1.31025299e-01 -2.17210442e-01\n",
      " -1.83343187e-01  5.65175712e-02 -2.93501634e-02  9.28365737e-02\n",
      "  2.96929702e-02  1.40830860e-01 -2.02926338e-01  5.64218834e-02\n",
      "  9.12641287e-02 -7.31113851e-02  4.71958667e-02  4.37190719e-02\n",
      " -8.56605396e-02 -2.47610118e-02  2.36367166e-01 -2.92902231e-01\n",
      "  2.13062353e-02 -4.05935645e-01 -1.07750334e-02 -3.05404574e-01\n",
      " -5.52320154e-03  1.99027404e-01 -9.52458382e-02 -2.10665613e-02\n",
      "  1.42497495e-01  6.30509928e-02  1.73044696e-01  2.19334751e-01\n",
      "  4.18797620e-02 -5.40908203e-02  2.92259812e-01  2.04791084e-01\n",
      "  8.78098235e-03 -8.24911892e-02  2.12526381e-01 -6.04261924e-03\n",
      "  1.16110511e-01  2.85938270e-02  9.86243933e-02 -1.01292223e-01\n",
      " -2.70887882e-01  1.14466809e-01 -2.79794991e-01  3.90979834e-02\n",
      " -3.95493247e-02 -7.76875019e-02 -2.35969007e-01 -3.92011881e-01\n",
      " -2.33059511e-01  4.63107303e-02  9.92574841e-02  1.41941175e-01\n",
      "  1.06477551e-01  6.05657212e-02  3.53271849e-02 -4.80017602e-01\n",
      " -6.25580549e-01  5.23960069e-02  3.06782246e-01  1.46656334e-01\n",
      "  9.89059284e-02  5.92881553e-02 -1.09411418e-01 -1.57191694e-01\n",
      " -3.74716669e-02 -1.86333925e-01 -3.13455164e-02 -1.19865097e-01\n",
      "  1.68296948e-01  6.80425018e-02 -3.72468159e-02  3.30539167e-01\n",
      "  2.88972557e-02 -5.65398186e-02  7.77614936e-02  2.01589197e-01\n",
      "  9.89681929e-02 -4.16375399e-02 -3.05554211e-01 -2.00881377e-01]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print(word_to_vec_model.wv.most_similar('الصحه'))\n",
    "print(word_to_vec_model.wv.most_similar('بالفيروس'))\n",
    "print(print(\"Feature of some words\\n\", word_to_vec_model['الصحه']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our word_features is: (200, 300)\n"
     ]
    }
   ],
   "source": [
    "# retrieve frequent words\n",
    "words = list(word_to_vec_model.wv.vocab)\n",
    "\n",
    "# I took just 100 words to present with PCA\n",
    "words = words[:200]\n",
    "\n",
    "# get the features of those 100 word\n",
    "word_features = word_to_vec_model[words] \n",
    "\n",
    "# it will be 100 word * 50 which 50 feature of each word\n",
    "print(\"The shape of our word_features is:\", word_features.shape)\n",
    "\n",
    "# Dimension reduction to display so instead of 50 dimension of each word \n",
    "#reduce to 2 dimensional that have fine display\n",
    "# from 50 features to just 2 features for displaying how words related words to gether\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# fit the model\n",
    "xy_words = pca.fit_transform(word_features)\n",
    "\n",
    "# set the visualization plot\n",
    "plt.style.use(['Solarize_Light2'])\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "# after reduction for all features in first column as x and all features from second column as y\n",
    "plt.scatter(xy_words[:, 0], xy_words[:, 1], marker='P',s=20, c=\"white\") \n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    # handle Arabic words to display from right to left and as complete word not just separate chars\n",
    "    word = arabic_reshaper.reshape(word) # handle arabic words on ploting\n",
    "    word = get_display(word)\n",
    "    # plot each word beside its point\n",
    "    plt.annotate(word, xy=(xy_words[i, 0], xy_words[i, 1]))\n",
    "plt.savefig('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

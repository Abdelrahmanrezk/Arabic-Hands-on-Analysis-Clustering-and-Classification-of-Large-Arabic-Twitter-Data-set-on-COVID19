{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designed report by Abdelrahman Rezk\n",
    "\n",
    "## COVID-19-Arabic-Tweets-Dataset\n",
    "\n",
    "\n",
    "**We have collected more than 3, 000, 000 tweets from twitter API, besides cleaning these tweets and we have make some analysis to get what is behind these tweets.**\n",
    "\n",
    "\n",
    "\n",
    "- Eng: Ayman Mahgoub\n",
    "- Researcher at electronics research institute\n",
    "- E-mail: <a href = \"mailto:Ayman_mhgb@hotmail.com\"> Ayman Mhgb </a>\n",
    "\n",
    "\n",
    "\n",
    "- Eng: Abdelrahman Rezk\n",
    "- Teaching Assistant at Arab Open University & NLP Engineer\n",
    "- E-mail: <a href = \"Abdelrahmanrezk12011@gmail.com\"> Abdelrahman Rezk </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corresponding Code Files\n",
    "\n",
    "**Files included in direction:**\n",
    "\n",
    "- config_files\n",
    "\n",
    "**Files Name:**\n",
    "\n",
    "- word2vec_anaysis.py\n",
    "\n",
    "- features_engineering.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------word2vec_anaysis File-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/word2vec_1.png\"  title=\"word2vec_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "Word embeddings are a modern approach for representing text in natural language processing.\n",
    "\n",
    "So we have used one of these techniques which is Word2Vec to knowing what is the most words describe our dataset based on the context of the words that appear in it.\n",
    "\n",
    "Unlike the usage of CountVectorizer or TF-idf models,which depends on the word count or the rare words and they depends on the Bag-Of-Words approach at the end. Word2Vec come to get an intuition about the words itself how it's related to other words from its category like words of [man, women], they have described different gender but have some of the features attach each other together, like the same context they appear in and the meaning is in one category of Gender and others like juices [Orange, Apple and so on].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We have impletent some of the function in this file and others we have used from previous work: **\n",
    "\n",
    "- dir_max_all_tweets\n",
    "- word_to_vec\n",
    "- save_words\n",
    "- Features Reduction Graph using PCA\n",
    "\n",
    "**For Features Extraction function**\n",
    "\n",
    "- tfidf_vectorizer_for_supervised_models\n",
    "- load_tfidf_vectorizer_for_supervised_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dir_max_all_tweets\n",
    "\n",
    "To build our Word2Vec model we need to know which review has the max number of words, this required to train the model for future work, but as for this function also we return the tweets of each file as a list of tweets each of these tweets is a list of words, as we can see ['عاجل', 'الولايات', 'المتحده', 'الامريكيه', 'تعلن', 'الحجر', 'الصحي', 'علي', '1000', 'شخص', 'كورونا'], this is required by word2vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word_to_vec\n",
    "\n",
    "**Moved from what model require to work with, to the model itself which also take these parameters: **\n",
    "\n",
    "### number_of_features\n",
    "\n",
    "The number of features we need for each word, and the number of features is that word represented in number of dimensions spaces.\n",
    "\n",
    "### window_size\n",
    "\n",
    "Then we will use window of 7 words as we know from the language model there is **Bigram** and others N-gram so we use 7-grams as our window which helps to detect for each word which context appears in as well as knowledge from other words beside this words which we have here 7 words before and 7 after.\n",
    "\n",
    "### min_words_count\n",
    "\n",
    "Here we tell the model to consider words that have appear a lot in the data and we have considered that the minimum count is 500 as our data have more than 200, 000 tweets in the first direction and more than 150, 000 tweets in the second direction.\n",
    "\n",
    "## Note 1\n",
    "We can see how models understand the words related to each other are in have most of the same features which represented in have a close location on the dimensions of the graph above.\n",
    "\n",
    "Also, we can notice that by:\n",
    "\n",
    "<img src=\"../../images/word2vec_2.png\"  title=\"word2vec_2\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save_words\n",
    "\n",
    "From this point and after we get the model work we need to save the words that appear a lot in our data in separated file to graph it with Tableau, besides of that we choose the most 50 words affect our dataset to graph it also, not just these words we consider also the frequency of these words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Reduction Graph using PCA\n",
    "\n",
    "From these words we get from the Word2Vec model we have graphed the first image in the report depends on the idea of Features Reduction as we have 300 features we can graph like these spaces so using Principal component analysis, we have reduced the features to just **2-feature**, and it will understand that the close features of different words will be looks like in one cluster as we can see in the graph above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshot of the Table after we get the most words affect the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"../../images/word2vec_3.png\" width=\"400\" height=\"400\" title=\"word2vec_3\">\n",
    "<td> <img src=\"../../images/word2vec_6.png\" width=\"400\" height=\"400\" title=\"word2vec_6\"> </td>\n",
    "</td>\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"../../images/word2vec_4.png\" width=\"400\" height=\"400\" title=\"word2vec_4\"> </td>\n",
    "\n",
    "<td> <img src=\"../../images/word2vec_5.png\" width=\"400\" height=\"400\" title=\"word2vec_5\"> </td>\n",
    "\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# -----------------------------features_engineering File-----------------------------\n",
    "\n",
    "The machine learning model can work with numbers not text data and for this point we need  to  provide  the  Models of machine learning the  data in  numbers  and  this  process  can  be done  by  what  is  called  features  extraction  and  there  are  different  Algorithms  of  these process one of them and the basic Algorithm is called One-Hot Vectorization and its work as for all of the words in the corps we used ordered the words using the alphabetic order and  give each  of  the indexes  from  0  to the last  word in  our  available  data and  for  this ordered the algorithm used to predict which words are similar to each other and thus lead to bad result because some words like [Orange & Apple] have the same meaning but the model will keep these words away of each other and make words like [Apple and again] to have similar meaning based on alphabetic order and for this we used another technique that helps us get a better result in training the machine learning algorithms like TF-IDF (term frequency & inverse term frequency)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidf_vectorizer_for_supervised_models\n",
    "\n",
    "The first part is TF that the term frequency is the number of times a word appears in a document divided by the total number of words in the document. Every document has its own term frequency. The second part is IDF inverse document frequency is the log of the number of documents divided by the number of documents that contain the word w. Inverse data frequency determines the weight of rare words across all documents in the corpus. Also, the TF-IDF makes these rare words have more weights because its repeat is not a big as other words like [they are and others] that may have a lot of time in our text so it takes the inverse after counting the number of each term over its document and overall corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_tfidf_vectorizer_for_supervised_models\n",
    "\n",
    "Just load the tf-idf model to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

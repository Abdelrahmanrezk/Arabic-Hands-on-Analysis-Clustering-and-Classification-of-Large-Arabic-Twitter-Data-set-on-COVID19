{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designed report by Abdelrahman Rezk\n",
    "\n",
    "## COVID-19-Arabic-Tweets-Dataset\n",
    "\n",
    "\n",
    "**We have collected more than 3, 000, 000 tweets from twitter API, besides cleaning these tweets and we have make some analysis to get what is behind these tweets.**\n",
    "\n",
    "\n",
    "\n",
    "- Eng: Ayman Mahgoub\n",
    "- Researcher at electronics research institute\n",
    "- E-mail: <a href = \"mailto:Ayman_mhgb@hotmail.com\"> Ayman Mhgb </a>\n",
    "\n",
    "\n",
    "\n",
    "- Eng: Abdelrahman Rezk\n",
    "- Teaching Assistant at Arab Open University & NLP Engineer\n",
    "- E-mail: <a href = \"Abdelrahmanrezk12011@gmail.com\"> Abdelrahman Rezk </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corresponding Code Files\n",
    "\n",
    "**Files included in direction:**\n",
    "\n",
    "- config_files\n",
    "- Tableau for graphs\n",
    "\n",
    "**Files Name:**\n",
    "\n",
    "- Analysis.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/direction_2_tweets_per_day.png\" width=\"600\" height=\"600\" title=\"direction_2_tweets_per_day\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words frequency\n",
    "\n",
    "Once we have collect and clean our dataset of some rubbish, we start the process of analysis these tweets, and one of these ways to analysis is to count the uniqe words to know which most of frequenct words that describe most of tweets.\n",
    "\n",
    "Designing the function that get the frequency of each word after of that we have graphed these words using Tableau.\n",
    "\n",
    "** We have impletent some of the function in this Analysis and others we have used from previous work: **\n",
    "\n",
    "- words_frequency\n",
    "- drop_rows\n",
    "- analysis_pipline\n",
    "- count_tweets_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## words_frequency\n",
    "\n",
    "Using the sklearn library and its method of countvectorizer which behave to count for each word in our data how this word frequency in our dataset, but based on our cleaning we have passed the list of all words of one direction which by we have introduce before its contain more than one file and for each file we have set of tweets, then from each file get all tweets, then append each of these tweets to one list, after that split all of tweets to words.\n",
    "\n",
    "After that we have initialize Dataframe that contain each word meet their frequency in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop_rows\n",
    "\n",
    "After the spliting process we have some of the words are numbers and some are in another language which not Arabic language so we have used some of the Analysis to handle like these cases and drop all of the words that are not Arabic words but it also have some of passed words during this process because of some complicated cases, but most of Non Arabic words are dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis_pipline\n",
    "\n",
    "Based on the idea of pipeline all of work in on way we have designed one function to throw all of the function will be used for this analysis like what we have discuess above others was from previous work and have their Docstring to get intuation about their work.\n",
    "\n",
    "Tehe other function used is: \n",
    "- read_direction_analysis >> and you can find in the direction_and_file_handleing notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count_tweets_file\n",
    "\n",
    "Because our data have different direction which represent different month that the tweets comes in, so we have get per file how much the tweets are, then we have a file for each direction represent these counts, after that we have graphed these counts Vs their days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Snapshots\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"../../images/direction__2_tweets_per_day.png\" width=\"400\" height=\"400\" title=\"direction__2_tweets_per_day\">\n",
    "</td>\n",
    "<td> <img src=\"../../images/direction_1_tweets_per_day.png\" width=\"400\" height=\"400\" title=\"direction_1_tweets_per_day\"> </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
